{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cccecc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8b1cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 00:25:43.565250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-07 00:25:44.398525: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-07 00:25:44.398648: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-07 00:25:44.398657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, transforms\n",
    "import random\n",
    "import pandas as pd\n",
    "# from Load_Recording_Data import LoadRecordingData, Recording  # (Use this for old data)\n",
    "from load_recording_data import LoadRecordingData#, Recording   # (Use this for new data)\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "import pysindy as ps\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.integrate import quad\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from fastdtw import fastdtw\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d690354",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpu_id = 0\n",
    "tf.config.set_visible_devices(gpus[gpu_id], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef74c03",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b41a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W5FebD1_10nM_Dofe_51_52_8000\n",
      "W5FebD1_10nM_Dofe_57_58_8000\n",
      "W5FebD1_10nM_Dofe_84_85_8000\n",
      "W8FebD2_10nM_Dofe_57_47_8000\n",
      "W8FebD2_10nM_Dofe_84_74_8000\n",
      "W8FebD3_10nM_Dofe_24_14_8000\n",
      "W8FebD3_10nM_Dofe_61_62_8000\n"
     ]
    }
   ],
   "source": [
    "intrasTrainVal, extrasTrainVal, intrasTest, extrasTest = LoadRecordingData()\n",
    "idx_train, idx_val = train_test_split(np.arange(len(intrasTrainVal)), test_size=0.20) #, random_state=42)\n",
    "\n",
    "extrasTrain = extrasTrainVal[idx_train]\n",
    "intrasTrain = intrasTrainVal[idx_train]\n",
    "extrasVal = extrasTrainVal[idx_val]\n",
    "intrasVal = intrasTrainVal[idx_val]\n",
    "\n",
    "\n",
    "# dataset = LoadRecordingData()\n",
    "# # idx_train, idx_test = train_test_split(np.arange(len(intrasRaw)), test_size=0.25, random_state=42)\n",
    "\n",
    "# extrasTrain = dataset['8k'].extras_training2\n",
    "# intrasTrain = dataset['8k'].intras_training2\n",
    "# extrasVal = dataset['8k'].extras_val2\n",
    "# intrasVal = dataset['8k'].intras_val2\n",
    "# extrasTest = dataset['8k'].extras_unseen2\n",
    "# intrasTest = dataset['8k'].intras_unseen2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d24f2",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88825bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a 1-dimensional array and a window size, and applies a moving\n",
    "# average filter to data.\n",
    "def moving_filter(data, window_size):\n",
    "    i = 0\n",
    "    moving_averages = []\n",
    "    while i < len(data) - window_size + 1:\n",
    "        this_window = data[i : i + window_size]\n",
    "\n",
    "        window_average = sum(this_window) / window_size\n",
    "        moving_averages.append(window_average)\n",
    "        i += 1\n",
    "\n",
    "    for i in range(window_size - 1):\n",
    "        ind = len(data) - (window_size - i)\n",
    "        moving_averages.append(np.mean(data[ind-window_size:ind]))\n",
    "\n",
    "    return moving_averages\n",
    "\n",
    "def mse(y, p):\n",
    "    return np.mean((y-p)**2)\n",
    "\n",
    "def mae(y, p):\n",
    "    return np.mean(np.absolute(y-p))\n",
    "\n",
    "def dtw(y, p):\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        dist, _ = fastdtw(y[i], p[i])\n",
    "        distances.append(dist)\n",
    "        \n",
    "    return np.mean(np.array(distances))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfa771",
   "metadata": {},
   "source": [
    "### Smooth the data by applying moving filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2680fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply moving filter on intras and extras\n",
    "# Training set\n",
    "windowSize = 20\n",
    "temp = []\n",
    "\n",
    "for rec in intrasTrain:\n",
    "    temp.append(moving_filter(rec, windowSize))\n",
    "    \n",
    "intrasTrain = np.array(temp)\n",
    "temp = []\n",
    "\n",
    "for rec in extrasTrain:\n",
    "    temp.append(moving_filter(rec, windowSize))\n",
    "    \n",
    "extrasTrain = np.array(temp)\n",
    "\n",
    "# Validation set\n",
    "temp = []\n",
    "\n",
    "for rec in intrasVal:\n",
    "    temp.append(moving_filter(rec, windowSize))\n",
    "    \n",
    "intrasVal = np.array(temp)\n",
    "temp = []\n",
    "\n",
    "for rec in extrasVal:\n",
    "    temp.append(moving_filter(rec, windowSize))\n",
    "    \n",
    "extrasVal = np.array(temp)\n",
    "\n",
    "\n",
    "# Test set\n",
    "temp = []\n",
    "\n",
    "for rec in intrasTest:\n",
    "    temp.append(moving_filter(rec, windowSize))\n",
    "    \n",
    "intrasTest = np.array(temp)\n",
    "temp = []\n",
    "\n",
    "for rec in extrasTest:\n",
    "    temp.append(moving_filter(rec, windowSize))\n",
    "    \n",
    "extrasTest = np.array(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad16963",
   "metadata": {},
   "source": [
    "### Function to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f2ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFeatures(intras, extras):\n",
    "    X = []\n",
    "    Y = []\n",
    "    windowSize = 20\n",
    "    \n",
    "    featureNames = ['d2eAP', 'deAP', 'eAP', 't'] #, 'exp(d2eAP)', 'exp(deAP)', 'exp(eAP)', 't']#, 'distFromMax', 'distFromMin']\n",
    "    s = [i for i in range(len(featureNames))]\n",
    "    combs = [list(itertools.combinations(s, r)) for r in range(len(s)+1)]\n",
    "    featureCombs = []\n",
    "    \n",
    "    for comb in combs[1:]:\n",
    "        for tpl in comb:\n",
    "            name = \"\"\n",
    "\n",
    "            for ind in tpl:\n",
    "                name += \"*\"+featureNames[ind]\n",
    "\n",
    "            featureCombs.append(name)\n",
    "            \n",
    "    t = np.array([i for i in range(len(extras[0]))])/len(extras[0])\n",
    "            \n",
    "    for i in range(len(extras)):\n",
    "        iAP = intras[i]\n",
    "        diAP = np.array(moving_filter(np.gradient(iAP), windowSize))\n",
    "        d2iAP = np.array(moving_filter(np.gradient(diAP), windowSize))\n",
    "\n",
    "        eAP = extras[i]\n",
    "        deAP = np.array(moving_filter(np.gradient(eAP), windowSize))\n",
    "        d2eAP = np.array(moving_filter(np.gradient(deAP), windowSize))\n",
    "            \n",
    "#         iAP = intras[i]\n",
    "#         diAP = np.gradient(iAP)\n",
    "#         d2iAP = np.gradient(diAP)\n",
    "\n",
    "#         eAP = extras[i]\n",
    "#         deAP = np.gradient(eAP)\n",
    "#         d2eAP = np.gradient(deAP)\n",
    "\n",
    "\n",
    "        featureList = [d2eAP, deAP, eAP, t] #, np.exp(d2eAP), np.exp(deAP), np.exp(eAP), t]\n",
    "    \n",
    "\n",
    "#         maxInd = np.argmax(eAP[:1500])\n",
    "#         minInd = 1500 + np.argmin(eAP[1500:])\n",
    "\n",
    "#         distFromMax = [i for i in range(len(extras[0]))]\n",
    "#         distFromMin = [i for i in range(len(extras[0]))]\n",
    "\n",
    "#         distFromMax = np.exp(-np.absolute(distFromMax - maxInd))\n",
    "#         distFromMin = np.exp(-np.absolute((distFromMin - minInd)/50))\n",
    "\n",
    "#         featureList.append(distFromMax)\n",
    "#         featureList.append(distFromMin)\n",
    "            \n",
    "        featureMatrix = []\n",
    "        \n",
    "        for comb in combs[1:]:\n",
    "            for tpl in comb:\n",
    "                feature = 1\n",
    "                \n",
    "                for ind in tpl:\n",
    "                    feature = feature*featureList[ind]\n",
    "                    \n",
    "                featureMatrix.append(feature)\n",
    "        \n",
    "        \n",
    "######### Uncomment the following to add a parameter for some finite timesteps\n",
    "######### around positive peak in eAP and local minima in latter half of eAP\n",
    "#         oneHotMatrix = []\n",
    "#         maxInd = np.argmax(eAP[:1500])\n",
    "#         minInd = 1500 + np.argmin(eAP[1500:])\n",
    "\n",
    "\n",
    "#         for j in range(maxInd-50, maxInd+50):\n",
    "#             temp = list(np.zeros(len(eAP)))\n",
    "#             temp[j] = 1\n",
    "#             oneHotMatrix.append(temp)\n",
    "\n",
    "#         for j in range(100):\n",
    "#             temp = list(np.zeros(len(eAP)))\n",
    "\n",
    "#             for i in range(minInd-500+10*j, minInd-500+10*j+10):\n",
    "#                 temp[i] = 1\n",
    "\n",
    "#             oneHotMatrix.append(temp)\n",
    "\n",
    "#         featureMatrix.extend(oneHotMatrix)\n",
    "        \n",
    "        X.extend(np.array(featureMatrix).T)\n",
    "        Y.extend(d2iAP)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "     \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1284b72",
   "metadata": {},
   "source": [
    "### Extract features from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c76b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, Y0 = GetFeatures(intrasTrain, extrasTrain)\n",
    "scaler0 = StandardScaler()\n",
    "# scalerY0 = StandardScaler()\n",
    "\n",
    "X0 = scaler0.fit_transform(X0)\n",
    "# Y0 = scalerY0.fit_transform(Y0.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f81bf",
   "metadata": {},
   "source": [
    "### Extract features from validation and test data for hyperparam tuning and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bdcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val0, Y_val0 = GetFeatures(intrasVal, extrasVal)\n",
    "X_val0 = scaler0.transform(X_val0)\n",
    "# Y_val0 = scalerY0.transform(Y_val0.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e6b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test0, Y_test0 = GetFeatures(intrasTest, extrasTest)\n",
    "X_test0 = scaler0.transform(X_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e01c7",
   "metadata": {},
   "source": [
    "### Estimate parameters of Physics loss equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ae1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell estimates parameters in physics loss equation using training data.\n",
    "# This equation is then used as a constraint in the loss function of the model.\n",
    "windowSize = 20\n",
    "\n",
    "v = intrasTrain\n",
    "dv = []\n",
    "d2v = []\n",
    "d3v = []\n",
    "\n",
    "for rec in v:\n",
    "    dv.append(moving_filter(np.gradient(rec), windowSize))\n",
    "\n",
    "dv = np.array(dv)\n",
    "\n",
    "for rec in dv:\n",
    "    d2v.append(moving_filter(np.gradient(rec), windowSize))\n",
    "\n",
    "d2v = np.array(d2v)\n",
    "\n",
    "for rec in d2v:\n",
    "    d3v.append(moving_filter(np.gradient(rec), windowSize))\n",
    "\n",
    "d3v = np.array(d3v)\n",
    "\n",
    "v = np.reshape(v, (-1))\n",
    "dv = np.reshape(dv, (-1))\n",
    "d2v = np.reshape(d2v, (-1))\n",
    "d3v = np.reshape(d3v, (-1))\n",
    "\n",
    "vFeatures = np.array([d2v*(v**2), v**3, dv*(v**2), v**4, v*d2v, v*dv, (d2v)**2, (d2v*dv), (v**3)*d2v, (v**3)*dv, v**5, (dv)**2, \n",
    "                      d2v, d3v*v, dv, np.ones(v.shape)]).T\n",
    "\n",
    "target = np.array([d3v*(v**2)]).T\n",
    "params = np.linalg.inv(vFeatures.T.dot(vFeatures)).dot(vFeatures.T.dot(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd997e8",
   "metadata": {},
   "source": [
    "### Linear regression model with custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b94096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(keras.layers.Layer):\n",
    "    def __init__(self, numParams, lamda):\n",
    "        super(LinearReg, self).__init__()\n",
    "        self.w = self.add_weight(shape=(numParams, 1), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.lamda = lamda\n",
    "        \n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b\n",
    "    \n",
    "    def loss_fn(self, output, target):\n",
    "        reg = self.lamda*(tf.math.reduce_sum(tf.math.abs(self.w)) + tf.math.reduce_sum(tf.math.abs(self.b)))\n",
    "        sqError = tf.math.reduce_sum(tf.math.square(output - target))\n",
    "        \n",
    "        d2v = tf.reshape(output, (-1)).numpy()\n",
    "        dv = np.cumsum(d2v)\n",
    "        v = np.cumsum(dv)\n",
    "#         d2v = np.gradient(dv)\n",
    "        d3v = np.gradient(d2v)\n",
    "        \n",
    "        vFeatures = np.array([d2v*(v**2), v**3, dv*(v**2), v**4, v*d2v, v*dv, (d2v)**2, (d2v*dv), (v**3)*d2v, (v**3)*dv, v**5, (dv)**2, \n",
    "                      d2v, d3v*v, dv, np.ones(v.shape)]).T\n",
    "        target = np.array([d3v*(v**2)]).T\n",
    "        \n",
    "        phyErr = tf.reduce_sum(tf.math.square(vFeatures.dot(params) - target))\n",
    "#         d1mse = tf.reduce_mean(tf.math.square(tf.math.cumsum(output, 0) - tf.math.cumsum(target, 0)))\n",
    "#         mse = tf.reduce_mean(tf.math.square(tf.math.cumsum(tf.math.cumsum(output, 0), 0) - tf.math.cumsum(tf.math.cumsum(target, 0), 0)))\n",
    "\n",
    "        return reg + sqError + tf.cast(phyErr, tf.float32)# + d1mse + mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b44d32",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e499b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  5  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  10  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  15  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  20  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  25  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  30  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  35  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  40  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n",
      "Epoch  45  complete with loss  tf.Tensor(inf, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "X = X0#[:, mask]\n",
    "Y = Y0\n",
    "X_val = X_val0#[:, mask]\n",
    "Y_val = Y_val0\n",
    "X_test = X_test0#[:, mask]\n",
    "Y_test = Y_test0\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-6, decay_steps=10000, decay_rate=0.95)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = len(extrasTrain[0])\n",
    "\n",
    "reg_selected = LinearReg(X.shape[1], 1)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    totalLoss = 0.0\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        loss= 0.0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = reg_selected(tf.convert_to_tensor(X[i:i+batch_size], dtype='float32'))\n",
    "            output = tf.reshape(output, (-1))\n",
    "            loss = reg_selected.loss_fn(output, tf.convert_to_tensor(Y[i:i+batch_size], dtype='float32'))\n",
    "            totalLoss += loss\n",
    "    \n",
    "        grads = tape.gradient(loss, reg_selected.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, reg_selected.trainable_weights))\n",
    "        \n",
    "#     totalLoss = 0.0\n",
    "#     for i in range(0, len(X), batch_size):\n",
    "#         loss= 0.0\n",
    "#         output = reg_selected(tf.convert_to_tensor(X[i:i+batch_size], dtype='float32'))\n",
    "#         output = tf.reshape(output, (-1))\n",
    "#         loss = reg_selected.loss_fn(output, tf.convert_to_tensor(Y[i:i+batch_size], dtype='float32'))\n",
    "#         totalLoss += loss\n",
    "            \n",
    "    if(epoch%5==0):\n",
    "        print(\"Epoch \", epoch, \" complete with loss \", totalLoss/len(X)*batch_size)\n",
    "\n",
    "# output = reg_selected(X_tensor)\n",
    "# output = tf.reshape(output, (-1)).numpy()\n",
    "    \n",
    "# print(\"MSE on train with model\", mse(output, Y))\n",
    "# print(\"MAE on train with model\", mae(output, Y))\n",
    "\n",
    "# valOut = reg_selected(tf.convert_to_tensor(X_val, dtype='float32'))\n",
    "# valOut = tf.reshape(valOut, (-1)).numpy()\n",
    "\n",
    "# print(\"MSE on val with model\", mse(valOut, Y_val))\n",
    "# print(\"MAE on val with model\", mae(valOut, Y_val))\n",
    "\n",
    "# print(reg_selected.w.numpy())\n",
    "# print(reg_selected.b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f4ffef",
   "metadata": {},
   "source": [
    "### Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cbf39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test d0 with reg_selected 24109285313.58502\n",
      "MAE on test d0 with reg_selected 103410.69331093924\n",
      "DTW on test d0 with reg_selected 827285494.6180615\n"
     ]
    }
   ],
   "source": [
    "valOut = reg_selected(tf.convert_to_tensor(X_val, dtype='float32'))\n",
    "\n",
    "predd2_selected = tf.reshape(valOut, (-1, 8000)).numpy()\n",
    "\n",
    "# predd1_selected = tf.reshape(valOut, (-1, 8000)).numpy()\n",
    "predd1_selected = np.cumsum(predd2_selected, axis=1)\n",
    "\n",
    "# prediAP_selected = tf.reshape(valOut, (-1, 8000)).numpy()\n",
    "prediAP_selected = np.cumsum(predd1_selected, axis=1)\n",
    "\n",
    "yiAP = intrasVal\n",
    "\n",
    "print(\"MSE on test d0 with reg_selected\", mse(prediAP_selected, yiAP))\n",
    "print(\"MAE on test d0 with reg_selected\", mae(prediAP_selected, yiAP))\n",
    "print(\"DTW on test d0 with reg_selected\", dtw(prediAP_selected, yiAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5db74",
   "metadata": {},
   "source": [
    "### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e0803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test d0 with reg_selected 36166548425.0116\n",
      "MAE on test d0 with reg_selected 133729.32522918348\n",
      "DTW on test d0 with reg_selected 1069834508.94318\n"
     ]
    }
   ],
   "source": [
    "testOut = reg_selected(tf.convert_to_tensor(X_test, dtype='float32'))\n",
    "\n",
    "predd2_selected = tf.reshape(testOut, (-1, 8000)).numpy()\n",
    "\n",
    "# predd1_selected = tf.reshape(testOut, (-1, 8000)).numpy()\n",
    "predd1_selected = np.cumsum(predd2_selected, axis=1)\n",
    "\n",
    "# prediAP_selected = tf.reshape(testOut, (-1, 8000)).numpy()\n",
    "prediAP_selected = np.cumsum(predd1_selected, axis=1)\n",
    "\n",
    "yiAP = intrasTest\n",
    "\n",
    "print(\"MSE on test d0 with reg_selected\", mse(prediAP_selected, yiAP))\n",
    "print(\"MAE on test d0 with reg_selected\", mae(prediAP_selected, yiAP))\n",
    "print(\"DTW on test d0 with reg_selected\", dtw(prediAP_selected, yiAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603a963",
   "metadata": {},
   "source": [
    "### Select features with high coefficient magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5261d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureWeights = np.absolute(np.reshape(reg_selected.w.numpy(), (-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65113b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = featureWeights >= np.sort(featureWeights)[-5]\n",
    "print(len(featureWeights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNames = ['d2eAP', 'deAP', 'eAP', 't']#, 'np.exp(d2eAP)', 'np.exp(deAP)', 'np.exp(eAP)', 't']#, 'distFromMax', 'distFromMin']\n",
    "s = [i for i in range(len(featureNames))]\n",
    "combs = [list(itertools.combinations(s, r)) for r in range(len(s)+1)]\n",
    "featureCombs = []\n",
    "\n",
    "for comb in combs[1:]:\n",
    "    for tpl in comb:\n",
    "        name = \"\"\n",
    "\n",
    "        for ind in tpl:\n",
    "            name += \"*\"+featureNames[ind]\n",
    "\n",
    "        featureCombs.append(name)\n",
    "\n",
    "#### Only for weight params\n",
    "# featureCombs.extend([\"max\"+str(i) for i in range(-50, 50)])\n",
    "# featureCombs.extend([\"min\"+str(i) for i in range(100)])\n",
    "####\n",
    "\n",
    "np.array(featureCombs)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70527a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_selected.w.numpy())\n",
    "print(reg_selected.b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54, 12, 40\n",
    "import pickle\n",
    "diAP_physics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68416b3",
   "metadata": {},
   "source": [
    "### Choose a recording at random from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2674b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 79#random.randint(0, len(extrasTest))#54, 12, 40, 308, 202\n",
    "print(ind)\n",
    "\n",
    "x, y = selector.transform(X_test)[ind*len(extrasTest[0]): (ind+1)*len(extrasTest[0])], Y_test[ind*len(extrasTest[0]): (ind+1)*len(extrasTest[0])]\n",
    "# x, y = X_test[ind*len(extrasTest[0]): (ind+1)*len(extrasTest[0])], Y_test[ind*len(extrasTest[0]): (ind+1)*len(extrasTest[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaef598",
   "metadata": {},
   "source": [
    "### Get model prediction for the chosen recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = tf.reshape(reg_selected(tf.convert_to_tensor(x, dtype='float32')), (-1)).numpy()\n",
    "pred = reg_selected.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1fdaa",
   "metadata": {},
   "source": [
    "### Prediction plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "transparency = 0.7\n",
    "fig0, ax0 = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "fig0.tight_layout(pad=5)\n",
    "\n",
    "# Plot second derivative\n",
    "ax0[0].plot(pred)\n",
    "ax0[0].plot(y, alpha=transparency)\n",
    "ax0[0].legend(['Prediction', 'Ground Truth'])\n",
    "ax0[0].title.set_text(\"First Derivative\")\n",
    "ax0[0].set_xlabel(\"Timesteps\")\n",
    "ax0[0].set_ylabel(\"diAP\")\n",
    "\n",
    "# Plot first derivative\n",
    "ax0[1].plot(np.cumsum(pred), linestyle='dashed')\n",
    "ax0[1].plot(moving_filter(np.gradient(intrasTest[ind]), windowSize), alpha=transparency)\n",
    "# ax0[1].plot(intrasTest[ind], alpha=transparency, color='red')\n",
    "ax0[1].legend(['Prediction', 'Ground Truth'])#, prop={'size': 18})\n",
    "ax0[1].title.set_text(\"Intra-cellular AP\")\n",
    "ax0[1].title.set_fontsize(20)\n",
    "ax0[1].set_xlabel(\"Timesteps\", fontsize=20)\n",
    "ax0[1].set_ylabel(\"iAP\", fontsize=20)\n",
    "# ax0[1].set_ylim([-0.2, 1.1])\n",
    "# ax0[1].tick_params(axis='x', labelsize=16)\n",
    "# ax0[1].tick_params(axis='y', labelsize=16)\n",
    "\n",
    "# Plot iAP\n",
    "ax0[2].plot(np.cumsum(np.cumsum(pred)))\n",
    "ax0[2].plot(intrasTest[ind], alpha=transparency)\n",
    "ax0[2].legend(['Prediction', 'Ground Truth'])\n",
    "ax0[2].title.set_text(\"Intra-cellular AP\")\n",
    "ax0[2].set_xlabel(\"Timesteps\")\n",
    "ax0[2].set_ylabel(\"iAP\")\n",
    "ax0[2].set_ylim([None, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diAP_physics[40] = [np.cumsum(pred), intrasTest[ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec741d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diAP_physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diAP_physics.pkl', 'wb') as fl:\n",
    "    pickle.dump(diAP_physics, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f83e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
